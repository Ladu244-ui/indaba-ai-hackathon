{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2516443a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Training data shape: (2351, 25)\n",
      "Testing data shape: (486, 25)\n"
     ]
    }
   ],
   "source": [
    "# Essential imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "train = pd.read_csv(\"../Data/Blood_samples_dataset_balanced_2(f).csv\")\n",
    "test = pd.read_csv(\"../Data/blood_samples_dataset_test.csv\")\n",
    "\n",
    "print(f\"Training data shape: {train.shape}\")\n",
    "print(f\"Testing data shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0542716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2351 entries, 0 to 2350\n",
      "Data columns (total 25 columns):\n",
      " #   Column                                     Non-Null Count  Dtype  \n",
      "---  ------                                     --------------  -----  \n",
      " 0   Glucose                                    2351 non-null   float64\n",
      " 1   Cholesterol                                2351 non-null   float64\n",
      " 2   Hemoglobin                                 2351 non-null   float64\n",
      " 3   Platelets                                  2351 non-null   float64\n",
      " 4   White Blood Cells                          2351 non-null   float64\n",
      " 5   Red Blood Cells                            2351 non-null   float64\n",
      " 6   Hematocrit                                 2351 non-null   float64\n",
      " 7   Mean Corpuscular Volume                    2351 non-null   float64\n",
      " 8   Mean Corpuscular Hemoglobin                2351 non-null   float64\n",
      " 9   Mean Corpuscular Hemoglobin Concentration  2351 non-null   float64\n",
      " 10  Insulin                                    2351 non-null   float64\n",
      " 11  BMI                                        2351 non-null   float64\n",
      " 12  Systolic Blood Pressure                    2351 non-null   float64\n",
      " 13  Diastolic Blood Pressure                   2351 non-null   float64\n",
      " 14  Triglycerides                              2351 non-null   float64\n",
      " 15  HbA1c                                      2351 non-null   float64\n",
      " 16  LDL Cholesterol                            2351 non-null   float64\n",
      " 17  HDL Cholesterol                            2351 non-null   float64\n",
      " 18  ALT                                        2351 non-null   float64\n",
      " 19  AST                                        2351 non-null   float64\n",
      " 20  Heart Rate                                 2351 non-null   float64\n",
      " 21  Creatinine                                 2351 non-null   float64\n",
      " 22  Troponin                                   2351 non-null   float64\n",
      " 23  C-reactive Protein                         2351 non-null   float64\n",
      " 24  Disease                                    2351 non-null   object \n",
      "dtypes: float64(24), object(1)\n",
      "memory usage: 459.3+ KB\n",
      "None\n",
      "\n",
      "Training data head:\n",
      "    Glucose  Cholesterol  Hemoglobin  Platelets  White Blood Cells  \\\n",
      "0  0.739597     0.650198    0.713631   0.868491           0.687433   \n",
      "1  0.121786     0.023058    0.944893   0.905372           0.507711   \n",
      "2  0.452539     0.116135    0.544560   0.400640           0.294538   \n",
      "3  0.136609     0.015605    0.419957   0.191487           0.081168   \n",
      "4  0.176737     0.752220    0.971779   0.785286           0.443880   \n",
      "\n",
      "   Red Blood Cells  Hematocrit  Mean Corpuscular Volume  \\\n",
      "0         0.529895    0.290006                 0.631045   \n",
      "1         0.403033    0.164216                 0.307553   \n",
      "2         0.382021    0.625267                 0.295122   \n",
      "3         0.166214    0.073293                 0.668719   \n",
      "4         0.439851    0.894991                 0.442159   \n",
      "\n",
      "   Mean Corpuscular Hemoglobin  Mean Corpuscular Hemoglobin Concentration  \\\n",
      "0                     0.001328                                   0.795829   \n",
      "1                     0.207938                                   0.505562   \n",
      "2                     0.868369                                   0.026808   \n",
      "3                     0.125447                                   0.501051   \n",
      "4                     0.257288                                   0.805987   \n",
      "\n",
      "   ...     HbA1c  LDL Cholesterol  HDL Cholesterol       ALT       AST  \\\n",
      "0  ...  0.502665         0.215560         0.512941  0.064187  0.610827   \n",
      "1  ...  0.856810         0.652465         0.106961  0.942549  0.344261   \n",
      "2  ...  0.466795         0.387332         0.421763  0.007186  0.506918   \n",
      "3  ...  0.016256         0.040137         0.826721  0.265415  0.594148   \n",
      "4  ...  0.429431         0.146294         0.221574  0.015280  0.567115   \n",
      "\n",
      "   Heart Rate  Creatinine  Troponin  C-reactive Protein   Disease  \n",
      "0    0.939485    0.095512  0.465957            0.769230   Healthy  \n",
      "1    0.666368    0.659060  0.816982            0.401166  Diabetes  \n",
      "2    0.431704    0.417295  0.799074            0.779208  Thalasse  \n",
      "3    0.225756    0.490349  0.637061            0.354094    Anemia  \n",
      "4    0.841412    0.153350  0.794008            0.094970  Thalasse  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "\n",
      "Target variable distribution:\n",
      "Disease\n",
      "Anemia      623\n",
      "Healthy     556\n",
      "Diabetes    540\n",
      "Thalasse    509\n",
      "Thromboc    123\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values:\n",
      "0\n",
      "\n",
      "Test data shape and missing values:\n",
      "Test shape: (486, 25)\n",
      "Test missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# Data Exploration and Understanding\n",
    "print(\"Training data info:\")\n",
    "print(train.info())\n",
    "print(\"\\nTraining data head:\")\n",
    "print(train.head())\n",
    "\n",
    "print(\"\\nTarget variable distribution:\")\n",
    "if 'Disease' in train.columns:\n",
    "    target_col = 'Disease'\n",
    "elif 'label' in train.columns:\n",
    "    target_col = 'label'\n",
    "else:\n",
    "    # Find the target column (usually the last column or named appropriately)\n",
    "    target_col = train.columns[-1]\n",
    "    print(f\"Assuming target column is: {target_col}\")\n",
    "\n",
    "print(train[target_col].value_counts())\n",
    "\n",
    "print(\"\\nMissing values:\")\n",
    "print(train.isnull().sum().sum())\n",
    "print(\"\\nTest data shape and missing values:\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"Test missing values: {test.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9dc46de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (2351, 24)\n",
      "Target shape: (2351,)\n",
      "Test features shape: (486, 25)\n",
      "Removing Disease from test data\n",
      "Test features shape after removing target: (486, 24)\n",
      "\n",
      "Training features: ['Glucose', 'Cholesterol', 'Hemoglobin', 'Platelets', 'White Blood Cells', 'Red Blood Cells', 'Hematocrit', 'Mean Corpuscular Volume', 'Mean Corpuscular Hemoglobin', 'Mean Corpuscular Hemoglobin Concentration', 'Insulin', 'BMI', 'Systolic Blood Pressure', 'Diastolic Blood Pressure', 'Triglycerides', 'HbA1c', 'LDL Cholesterol', 'HDL Cholesterol', 'ALT', 'AST', 'Heart Rate', 'Creatinine', 'Troponin', 'C-reactive Protein']\n",
      "Test features: ['Glucose', 'Cholesterol', 'Hemoglobin', 'Platelets', 'White Blood Cells', 'Red Blood Cells', 'Hematocrit', 'Mean Corpuscular Volume', 'Mean Corpuscular Hemoglobin', 'Mean Corpuscular Hemoglobin Concentration', 'Insulin', 'BMI', 'Systolic Blood Pressure', 'Diastolic Blood Pressure', 'Triglycerides', 'HbA1c', 'LDL Cholesterol', 'HDL Cholesterol', 'ALT', 'AST', 'Heart Rate', 'Creatinine', 'Troponin', 'C-reactive Protein']\n",
      "\n",
      "Aligned shapes - Train: (2351, 24), Test: (486, 24)\n",
      "\n",
      "Target classes: ['Anemia' 'Diabetes' 'Healthy' 'Thalasse' 'Thromboc']\n",
      "Encoded target distribution: [623 540 556 509 123]\n",
      "\n",
      "Scaled features shape: (2351, 24)\n",
      "Scaled test features shape: (486, 24)\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "# Separate features and target\n",
    "X = train.drop(target_col, axis=1)\n",
    "y = train[target_col]\n",
    "\n",
    "# Handle test data (assuming it doesn't have target column)\n",
    "X_test = test.copy()\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Test features shape: {X_test.shape}\")\n",
    "\n",
    "# Check if test data has target column and remove it\n",
    "if target_col in X_test.columns:\n",
    "    print(f\"Removing {target_col} from test data\")\n",
    "    X_test = X_test.drop(target_col, axis=1)\n",
    "    print(f\"Test features shape after removing target: {X_test.shape}\")\n",
    "\n",
    "# Check feature alignment\n",
    "print(f\"\\nTraining features: {list(X.columns)}\")\n",
    "print(f\"Test features: {list(X_test.columns)}\")\n",
    "\n",
    "# Align features between train and test\n",
    "missing_in_test = set(X.columns) - set(X_test.columns)\n",
    "missing_in_train = set(X_test.columns) - set(X.columns)\n",
    "\n",
    "if missing_in_test:\n",
    "    print(f\"Features missing in test: {missing_in_test}\")\n",
    "    # Add missing columns with zeros\n",
    "    for col in missing_in_test:\n",
    "        X_test[col] = 0\n",
    "\n",
    "if missing_in_train:\n",
    "    print(f\"Features missing in train: {missing_in_train}\")\n",
    "    # Remove extra columns from test\n",
    "    X_test = X_test.drop(columns=missing_in_train)\n",
    "\n",
    "# Reorder columns to match training data\n",
    "X_test = X_test[X.columns]\n",
    "\n",
    "print(f\"\\nAligned shapes - Train: {X.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Encode categorical target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"\\nTarget classes: {label_encoder.classes_}\")\n",
    "print(f\"Encoded target distribution: {np.bincount(y_encoded)}\")\n",
    "\n",
    "# Handle any remaining missing values\n",
    "if X.isnull().sum().sum() > 0:\n",
    "    print(\"Filling missing values in training data...\")\n",
    "    X = X.fillna(X.median())\n",
    "    \n",
    "if X_test.isnull().sum().sum() > 0:\n",
    "    print(\"Filling missing values in test data...\")\n",
    "    X_test = X_test.fillna(X_test.median())\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nScaled features shape: {X_scaled.shape}\")\n",
    "print(f\"Scaled test features shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94169743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 1880 samples\n",
      "Validation set: 471 samples\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Better Data Splitting and Validation\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Split your training data into train/validation sets\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_scaled, y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train_split.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val_split.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b286c84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:1.50382\n",
      "[10]\tvalidation_0-mlogloss:0.83284\n",
      "[20]\tvalidation_0-mlogloss:0.51282\n",
      "[30]\tvalidation_0-mlogloss:0.32427\n",
      "[40]\tvalidation_0-mlogloss:0.21587\n",
      "[50]\tvalidation_0-mlogloss:0.14556\n",
      "[60]\tvalidation_0-mlogloss:0.10130\n",
      "[70]\tvalidation_0-mlogloss:0.07336\n",
      "[80]\tvalidation_0-mlogloss:0.05517\n",
      "[90]\tvalidation_0-mlogloss:0.04389\n",
      "[99]\tvalidation_0-mlogloss:0.03780\n",
      "\n",
      "Validation Accuracy: 1.0000\n",
      "Training Accuracy: 1.0000\n",
      "Overfitting Gap: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Regularized XGBoost Model with Early Stopping\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# More conservative hyperparameters to prevent overfitting\n",
    "xgb_regularized = XGBClassifier(\n",
    "    n_estimators=100,          # Reduced from 200\n",
    "    learning_rate=0.05,        # Reduced from 0.1\n",
    "    max_depth=4,               # Reduced from 6\n",
    "    min_child_weight=3,        # Increased from 1\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=1,               # L1 regularization\n",
    "    reg_lambda=1,              # L2 regularization\n",
    "    gamma=1,                   # Minimum split loss\n",
    "    objective='multi:softprob',\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    early_stopping_rounds=20   # Stop if no improvement for 20 rounds\n",
    ")\n",
    "\n",
    "# Train with early stopping\n",
    "xgb_regularized.fit(\n",
    "    X_train_split, y_train_split,\n",
    "    eval_set=[(X_val_split, y_val_split)],\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_preds = xgb_regularized.predict(X_val_split)\n",
    "val_accuracy = accuracy_score(y_val_split, val_preds)\n",
    "print(f\"\\nValidation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Check training accuracy to see overfitting\n",
    "train_preds = xgb_regularized.predict(X_train_split)\n",
    "train_accuracy = accuracy_score(y_train_split, train_preds)\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Overfitting Gap: {train_accuracy - val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c623b0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [1. 1. 1. 1. 1.]\n",
      "Mean CV accuracy: 1.0000 (+/- 0.0000)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Cross-Validation for Better Evaluation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Create a new XGBoost model without early stopping for cross-validation\n",
    "xgb_cv = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    min_child_weight=3,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=1,\n",
    "    reg_lambda=1,\n",
    "    gamma=1,\n",
    "    objective='multi:softprob',\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42,\n",
    "    use_label_encoder=False\n",
    "    # No early_stopping_rounds for cross-validation\n",
    ")\n",
    "\n",
    "cv_scores = cross_val_score(\n",
    "    xgb_cv, X_scaled, y_encoded, \n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(f\"Cross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean CV accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8b90b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Results:\n",
      "Training Accuracy: 1.0000\n",
      "Validation Accuracy: 1.0000\n",
      "Overfitting Gap: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Alternative - Random Forest (Often Less Prone to Overfitting)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Evaluate Random Forest\n",
    "rf_val_preds = rf_model.predict(X_val_split)\n",
    "rf_val_accuracy = accuracy_score(y_val_split, rf_val_preds)\n",
    "rf_train_preds = rf_model.predict(X_train_split)\n",
    "rf_train_accuracy = accuracy_score(y_train_split, rf_train_preds)\n",
    "\n",
    "print(f\"\\nRandom Forest Results:\")\n",
    "print(f\"Training Accuracy: {rf_train_accuracy:.4f}\")\n",
    "print(f\"Validation Accuracy: {rf_val_accuracy:.4f}\")\n",
    "print(f\"Overfitting Gap: {rf_train_accuracy - rf_val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7d2518f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensemble Validation Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Ensemble Method (Often More Robust)\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create ensemble of different models\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# Use the cross-validation XGBoost model (without early stopping) for ensemble\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', xgb_cv),  # Use xgb_cv instead of xgb_regularized\n",
    "        ('rf', rf_model),\n",
    "        ('lr', log_reg)\n",
    "    ],\n",
    "    voting='soft'  # Use probability averages\n",
    ")\n",
    "\n",
    "ensemble.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Evaluate ensemble\n",
    "ensemble_val_preds = ensemble.predict(X_val_split)\n",
    "ensemble_val_accuracy = accuracy_score(y_val_split, ensemble_val_preds)\n",
    "print(f\"\\nEnsemble Validation Accuracy: {ensemble_val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "587788ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Selected Model Validation Accuracy: 1.0000\n",
      "Selected features: ['Glucose', 'Cholesterol', 'Platelets', 'White Blood Cells', 'Red Blood Cells', 'Hematocrit', 'Mean Corpuscular Volume', 'Mean Corpuscular Hemoglobin', 'Insulin', 'BMI', 'HbA1c', 'HDL Cholesterol', 'ALT', 'Heart Rate', 'Troponin']\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Feature Selection to Reduce Overfitting\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Select top K features\n",
    "k_best = SelectKBest(score_func=f_classif, k=15)  # Select top 15 features\n",
    "X_train_selected = k_best.fit_transform(X_train_split, y_train_split)\n",
    "X_val_selected = k_best.transform(X_val_split)\n",
    "\n",
    "# Train model on selected features\n",
    "xgb_selected = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    min_child_weight=3,\n",
    "    reg_alpha=1,\n",
    "    reg_lambda=1,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "xgb_selected.fit(X_train_selected, y_train_split)\n",
    "\n",
    "# Evaluate\n",
    "val_preds_selected = xgb_selected.predict(X_val_selected)\n",
    "val_acc_selected = accuracy_score(y_val_split, val_preds_selected)\n",
    "print(f\"\\nFeature Selected Model Validation Accuracy: {val_acc_selected:.4f}\")\n",
    "\n",
    "# Show selected features\n",
    "selected_features = X.columns[k_best.get_support()]\n",
    "print(f\"Selected features: {list(selected_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24b656f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using XGBoost Regularized for final predictions\n",
      "Submission saved with validation accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Generate Final Predictions with Best Model\n",
    "# Choose the model with best validation performance and smallest overfitting gap\n",
    "\n",
    "# Select the best model based on validation performance\n",
    "models = {\n",
    "    'XGBoost Regularized': (xgb_regularized, X_test_scaled),\n",
    "    'Random Forest': (rf_model, X_test_scaled),\n",
    "    'Ensemble': (ensemble, X_test_scaled),\n",
    "    'Feature Selected': (xgb_selected, k_best.transform(X_test_scaled))\n",
    "}\n",
    "\n",
    "best_model_name = 'XGBoost Regularized'  # You can change this based on results\n",
    "best_model, X_test_final = models[best_model_name]\n",
    "\n",
    "print(f\"Using {best_model_name} for final predictions\")\n",
    "\n",
    "# Generate predictions\n",
    "final_preds = best_model.predict(X_test_final)\n",
    "final_preds_labels = label_encoder.inverse_transform(final_preds)\n",
    "\n",
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'id': range(1, len(final_preds_labels) + 1),\n",
    "    'label': final_preds_labels\n",
    "})\n",
    "\n",
    "submission.to_csv(\"../outputs/submission_regularized.csv\", index=False)\n",
    "print(f\"Submission saved with validation accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "700b6b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ADDRESSING OVERFITTING ISSUES\n",
      "==================================================\n",
      "Checking for potential data leakage...\n",
      "\n",
      "Training with more extreme regularization...\n",
      "New validation split accuracy with simpler model: 0.9427\n",
      "Created submission with extremely regularized model\n",
      "\n",
      "Top 10 important features:\n",
      "                        feature  importance\n",
      "8   Mean Corpuscular Hemoglobin    0.070073\n",
      "6                    Hematocrit    0.065316\n",
      "4             White Blood Cells    0.061285\n",
      "17              HDL Cholesterol    0.059071\n",
      "19                          AST    0.058462\n",
      "22                     Troponin    0.052791\n",
      "7       Mean Corpuscular Volume    0.050955\n",
      "15                        HbA1c    0.047888\n",
      "20                   Heart Rate    0.045035\n",
      "3                     Platelets    0.041715\n",
      "\n",
      "Training with only top 10 features...\n",
      "Validation accuracy with top features: 0.9533\n"
     ]
    }
   ],
   "source": [
    "# Improved approach to fix overfitting\n",
    "print(\"=\"*50)\n",
    "print(\"ADDRESSING OVERFITTING ISSUES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. First, let's check if there are any common IDs or clear data leakage between train and test\n",
    "print(\"Checking for potential data leakage...\")\n",
    "if 'id' in train.columns and 'id' in test.columns:\n",
    "    common_ids = set(train['id']) & set(test['id'])\n",
    "    print(f\"Number of common IDs between train and test: {len(common_ids)}\")\n",
    "    if len(common_ids) > 0:\n",
    "        print(\"WARNING: Found common IDs between train and test - possible data leakage!\")\n",
    "\n",
    "# 2. Try more extreme regularization and simpler model\n",
    "print(\"\\nTraining with more extreme regularization...\")\n",
    "xgb_extreme_reg = XGBClassifier(\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=3,\n",
    "    min_child_weight=5,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    reg_alpha=10,\n",
    "    reg_lambda=10,\n",
    "    gamma=3,\n",
    "    objective='multi:softprob',\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42,\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "# Train with the full training set (for final model)\n",
    "xgb_extreme_reg.fit(X_scaled, y_encoded)\n",
    "\n",
    "# 3. Create a new test-train split with shuffling to check if it's a data ordering issue\n",
    "X_train_new, X_val_new, y_train_new, y_val_new = train_test_split(\n",
    "    X_scaled, y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=99,  # Different random state\n",
    "    stratify=y_encoded,\n",
    "    shuffle=True  # Explicitly shuffle\n",
    ")\n",
    "\n",
    "# Train a simpler model on this new split\n",
    "xgb_simple = XGBClassifier(\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=3,\n",
    "    min_child_weight=5,\n",
    "    reg_alpha=10,\n",
    "    reg_lambda=10,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "xgb_simple.fit(X_train_new, y_train_new)\n",
    "val_preds_simple = xgb_simple.predict(X_val_new)\n",
    "val_acc_simple = accuracy_score(y_val_new, val_preds_simple)\n",
    "print(f\"New validation split accuracy with simpler model: {val_acc_simple:.4f}\")\n",
    "\n",
    "# 4. Create a submission with the extremely regularized model\n",
    "final_preds_extreme = xgb_extreme_reg.predict(X_test_scaled)\n",
    "final_preds_labels_extreme = label_encoder.inverse_transform(final_preds_extreme)\n",
    "\n",
    "submission_extreme = pd.DataFrame({\n",
    "    'id': range(1, len(final_preds_labels_extreme) + 1),\n",
    "    'label': final_preds_labels_extreme\n",
    "})\n",
    "\n",
    "submission_extreme.to_csv(\"../outputs/submission_extreme_reg.csv\", index=False)\n",
    "print(f\"Created submission with extremely regularized model\")\n",
    "\n",
    "# 5. Check feature importance for potential problematic features\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': xgb_extreme_reg.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 important features:\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# 6. Try using only the top important features\n",
    "top_features = feature_importance.head(10)['feature'].tolist()\n",
    "print(f\"\\nTraining with only top {len(top_features)} features...\")\n",
    "\n",
    "X_top = X[top_features]\n",
    "X_test_top = X_test[top_features]\n",
    "\n",
    "# Scale these features\n",
    "scaler_top = StandardScaler()\n",
    "X_top_scaled = scaler_top.fit_transform(X_top)\n",
    "X_test_top_scaled = scaler_top.transform(X_test_top)\n",
    "\n",
    "# Train a model with only top features\n",
    "xgb_top = XGBClassifier(\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=3,\n",
    "    min_child_weight=5,\n",
    "    reg_alpha=10,\n",
    "    reg_lambda=10,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "# Split with top features\n",
    "X_train_top, X_val_top, y_train_top, y_val_top = train_test_split(\n",
    "    X_top_scaled, y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=101,\n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "xgb_top.fit(X_train_top, y_train_top)\n",
    "val_preds_top = xgb_top.predict(X_val_top)\n",
    "val_acc_top = accuracy_score(y_val_top, val_preds_top)\n",
    "print(f\"Validation accuracy with top features: {val_acc_top:.4f}\")\n",
    "\n",
    "# Make predictions with this model\n",
    "final_preds_top = xgb_top.predict(X_test_top_scaled)\n",
    "final_preds_labels_top = label_encoder.inverse_transform(final_preds_top)\n",
    "\n",
    "submission_top = pd.DataFrame({\n",
    "    'id': range(1, len(final_preds_labels_top) + 1),\n",
    "    'label': final_preds_labels_top\n",
    "})\n",
    "\n",
    "submission_top.to_csv(\"../outputs/submission_top_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0644a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ADDITIONAL ANTI-OVERFITTING STRATEGIES\n",
      "==================================================\n",
      "Checking for potential duplicates or unusual patterns...\n",
      "Number of samples with very similar neighbors: 2351\n",
      "This could indicate near-duplicates in the training data\n",
      "\n",
      "Trying a gradient boosting model with different implementation...\n",
      "Validation accuracy with GradientBoostingClassifier: 0.9809\n",
      "Created submission with GradientBoostingClassifier\n",
      "\n",
      "Trying data augmentation with noise...\n",
      "Validation accuracy with augmented data: 1.0000\n",
      "Created submission with augmented data model\n",
      "\n",
      "You now have multiple new submissions to try!\n"
     ]
    }
   ],
   "source": [
    "# Additional strategies to improve model generalization\n",
    "print(\"=\"*50)\n",
    "print(\"ADDITIONAL ANTI-OVERFITTING STRATEGIES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Data inspection - check for near-duplicates or unusual patterns\n",
    "print(\"Checking for potential duplicates or unusual patterns...\")\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Look for very similar samples using nearest neighbors\n",
    "nn = NearestNeighbors(n_neighbors=2)\n",
    "nn.fit(X_scaled)\n",
    "distances, indices = nn.kneighbors(X_scaled)\n",
    "\n",
    "# The first nearest neighbor is the point itself (distance=0)\n",
    "# The second nearest is the closest different point\n",
    "closest_distances = distances[:, 1]\n",
    "very_close_samples = np.where(closest_distances < 0.1)[0]\n",
    "print(f\"Number of samples with very similar neighbors: {len(very_close_samples)}\")\n",
    "\n",
    "if len(very_close_samples) > 0:\n",
    "    print(\"This could indicate near-duplicates in the training data\")\n",
    "\n",
    "# 2. Try a completely different model type\n",
    "print(\"\\nTrying a gradient boosting model with different implementation...\")\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=3,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    subsample=0.7,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train_gb, X_val_gb, y_train_gb, y_val_gb = train_test_split(\n",
    "    X_scaled, y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=123,\n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "gb_model.fit(X_train_gb, y_train_gb)\n",
    "val_preds_gb = gb_model.predict(X_val_gb)\n",
    "val_acc_gb = accuracy_score(y_val_gb, val_preds_gb)\n",
    "print(f\"Validation accuracy with GradientBoostingClassifier: {val_acc_gb:.4f}\")\n",
    "\n",
    "# Create a submission with this model\n",
    "final_preds_gb = gb_model.predict(X_test_scaled)\n",
    "final_preds_labels_gb = label_encoder.inverse_transform(final_preds_gb)\n",
    "\n",
    "submission_gb = pd.DataFrame({\n",
    "    'id': range(1, len(final_preds_labels_gb) + 1),\n",
    "    'label': final_preds_labels_gb\n",
    "})\n",
    "\n",
    "submission_gb.to_csv(\"../outputs/submission_gb.csv\", index=False)\n",
    "print(f\"Created submission with GradientBoostingClassifier\")\n",
    "\n",
    "# 3. Try data augmentation with noise to improve generalization\n",
    "print(\"\\nTrying data augmentation with noise...\")\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Create a noisy version of the training data\n",
    "np.random.seed(42)\n",
    "noise_factor = 0.01  # Add 1% noise\n",
    "X_noisy = X_scaled + np.random.normal(0, noise_factor, X_scaled.shape)\n",
    "\n",
    "# Combine original and noisy data\n",
    "X_augmented = np.vstack([X_scaled, X_noisy])\n",
    "y_augmented = np.hstack([y_encoded, y_encoded])\n",
    "\n",
    "# Train a model on the augmented data\n",
    "xgb_augmented = XGBClassifier(\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=3,\n",
    "    min_child_weight=5,\n",
    "    reg_alpha=5,\n",
    "    reg_lambda=5,\n",
    "    gamma=1,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "# New validation split with augmented data\n",
    "X_train_aug, X_val_aug, y_train_aug, y_val_aug = train_test_split(\n",
    "    X_augmented, y_augmented, \n",
    "    test_size=0.1,  # Smaller validation set\n",
    "    random_state=987,\n",
    "    stratify=y_augmented\n",
    ")\n",
    "\n",
    "xgb_augmented.fit(X_train_aug, y_train_aug)\n",
    "\n",
    "# Evaluate on original validation data to avoid noise in evaluation\n",
    "X_pure_val, _, y_pure_val, _ = train_test_split(\n",
    "    X_scaled, y_encoded, \n",
    "    test_size=0.9,  # Take 10% for clean validation\n",
    "    random_state=987,\n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "val_preds_aug = xgb_augmented.predict(X_pure_val)\n",
    "val_acc_aug = accuracy_score(y_pure_val, val_preds_aug)\n",
    "print(f\"Validation accuracy with augmented data: {val_acc_aug:.4f}\")\n",
    "\n",
    "# Create submission with augmented model\n",
    "final_preds_aug = xgb_augmented.predict(X_test_scaled)\n",
    "final_preds_labels_aug = label_encoder.inverse_transform(final_preds_aug)\n",
    "\n",
    "submission_aug = pd.DataFrame({\n",
    "    'id': range(1, len(final_preds_labels_aug) + 1),\n",
    "    'label': final_preds_labels_aug\n",
    "})\n",
    "\n",
    "submission_aug.to_csv(\"../outputs/submission_augmented.csv\", index=False)\n",
    "print(f\"Created submission with augmented data model\")\n",
    "\n",
    "print(\"\\nYou now have multiple new submissions to try!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8307dfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "FINAL ROBUST MODELING APPROACH\n",
      "==================================================\n",
      "Inspecting features for potential identifiers or leakage...\n",
      "Features with lowest variance (potential constants or near-constants):\n",
      "              feature  variance\n",
      "21         Creatinine       1.0\n",
      "20         Heart Rate       1.0\n",
      "4   White Blood Cells       1.0\n",
      "0             Glucose       1.0\n",
      "19                AST       1.0\n",
      "\n",
      "Features with highest variance (potential identifiers or noisy features):\n",
      "                     feature  variance\n",
      "13  Diastolic Blood Pressure       1.0\n",
      "7    Mean Corpuscular Volume       1.0\n",
      "15                     HbA1c       1.0\n",
      "16           LDL Cholesterol       1.0\n",
      "5            Red Blood Cells       1.0\n",
      "\n",
      "Creating a robust ensemble of diverse models...\n",
      "\n",
      "Training with a curated subset of features...\n",
      "Using 18 curated features: ['Glucose', 'Cholesterol', 'Hemoglobin', 'Platelets', 'Hematocrit', 'Mean Corpuscular Volume', 'Mean Corpuscular Hemoglobin', 'Mean Corpuscular Hemoglobin Concentration', 'Insulin', 'BMI', 'Systolic Blood Pressure', 'Diastolic Blood Pressure', 'Triglycerides', 'HDL Cholesterol', 'ALT', 'AST', 'Troponin', 'C-reactive Protein']\n",
      "Created submission with robust ensemble on curated features\n",
      "\n",
      "Creating a final model using only key biological markers...\n",
      "Using only 7 essential biological markers: ['Hemoglobin', 'Red Blood Cells', 'White Blood Cells', 'Platelets', 'Glucose', 'HbA1c', 'Insulin']\n",
      "Created submission using only essential biological markers\n",
      "\n",
      "You now have a total of 7 new submission files to try!\n",
      "1. submission_extreme_reg.csv - XGBoost with extreme regularization\n",
      "2. submission_top_features.csv - XGBoost with only top 10 important features\n",
      "3. submission_gb.csv - GradientBoostingClassifier model\n",
      "4. submission_augmented.csv - XGBoost with data augmentation\n",
      "5. submission_robust_ensemble.csv - Ensemble of diverse models on curated features\n",
      "6. submission_key_markers.csv - GradientBoosting with only essential biological markers\n",
      "7. Your original regularized model for comparison\n"
     ]
    }
   ],
   "source": [
    "# Final approach: Handling dataset issues and creating robust models\n",
    "print(\"=\"*50)\n",
    "print(\"FINAL ROBUST MODELING APPROACH\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Let's first check if there are any potential identifier columns or data leakage issues\n",
    "print(\"Inspecting features for potential identifiers or leakage...\")\n",
    "\n",
    "# Calculate variance of each feature to find potential ID-like columns\n",
    "feature_variance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'variance': np.var(X_scaled, axis=0)\n",
    "}).sort_values('variance', ascending=True)\n",
    "\n",
    "print(\"Features with lowest variance (potential constants or near-constants):\")\n",
    "print(feature_variance.head(5))\n",
    "\n",
    "print(\"\\nFeatures with highest variance (potential identifiers or noisy features):\")\n",
    "print(feature_variance.tail(5))\n",
    "\n",
    "# 2. Create a robust ensemble of different model types with high regularization\n",
    "print(\"\\nCreating a robust ensemble of diverse models...\")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "# Very strong regularization for all models\n",
    "logistic = LogisticRegression(C=0.1, max_iter=1000, random_state=42)\n",
    "xgb_final = XGBClassifier(\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=2,\n",
    "    min_child_weight=10,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_alpha=20,\n",
    "    reg_lambda=20,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False\n",
    ")\n",
    "rf_final = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=5,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "gb_final = GradientBoostingClassifier(\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=2,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    subsample=0.6,\n",
    "    random_state=42\n",
    ")\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Create the ensemble with hard voting (majority rule)\n",
    "# Hard voting is more robust to overfitting than soft voting\n",
    "final_ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', logistic),\n",
    "        ('xgb', xgb_final),\n",
    "        ('rf', rf_final),\n",
    "        ('gb', gb_final),\n",
    "        ('gnb', gnb)\n",
    "    ],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "# 3. Train on a subset of features (removing potential problematic ones)\n",
    "print(\"\\nTraining with a curated subset of features...\")\n",
    "\n",
    "# Remove features with extremely high or low variance\n",
    "low_var_features = feature_variance.head(3)['feature'].tolist()\n",
    "high_var_features = feature_variance.tail(3)['feature'].tolist()\n",
    "problematic_features = low_var_features + high_var_features\n",
    "\n",
    "curated_features = [f for f in X.columns if f not in problematic_features]\n",
    "print(f\"Using {len(curated_features)} curated features: {curated_features}\")\n",
    "\n",
    "X_curated = X[curated_features]\n",
    "X_test_curated = X_test[curated_features]\n",
    "\n",
    "# Scale these features\n",
    "scaler_curated = StandardScaler()\n",
    "X_curated_scaled = scaler_curated.fit_transform(X_curated)\n",
    "X_test_curated_scaled = scaler_curated.transform(X_test_curated)\n",
    "\n",
    "# Train the ensemble on the curated features\n",
    "final_ensemble.fit(X_curated_scaled, y_encoded)\n",
    "\n",
    "# 4. Create a submission with this robust approach\n",
    "final_preds_robust = final_ensemble.predict(X_test_curated_scaled)\n",
    "final_preds_labels_robust = label_encoder.inverse_transform(final_preds_robust)\n",
    "\n",
    "submission_robust = pd.DataFrame({\n",
    "    'id': range(1, len(final_preds_labels_robust) + 1),\n",
    "    'label': final_preds_labels_robust\n",
    "})\n",
    "\n",
    "submission_robust.to_csv(\"../outputs/submission_robust_ensemble.csv\", index=False)\n",
    "print(f\"Created submission with robust ensemble on curated features\")\n",
    "\n",
    "# 5. One more attempt using only minimal set of the most important biological features\n",
    "# These are features that should be most predictive for the actual diseases based on domain knowledge\n",
    "print(\"\\nCreating a final model using only key biological markers...\")\n",
    "\n",
    "key_markers = [\n",
    "    'Hemoglobin',         # Key for Anemia and Thalassemia\n",
    "    'Red Blood Cells',    # Key for Anemia and Thalassemia\n",
    "    'White Blood Cells',  # Important for infections and immunity\n",
    "    'Platelets',          # Key for Thrombocytopenia\n",
    "    'Glucose',            # Key for Diabetes\n",
    "    'HbA1c',              # Key for Diabetes\n",
    "    'Insulin'             # Key for Diabetes\n",
    "]\n",
    "\n",
    "print(f\"Using only {len(key_markers)} essential biological markers: {key_markers}\")\n",
    "\n",
    "X_key = X[key_markers]\n",
    "X_test_key = X_test[key_markers]\n",
    "\n",
    "# Scale these features\n",
    "scaler_key = StandardScaler()\n",
    "X_key_scaled = scaler_key.fit_transform(X_key)\n",
    "X_test_key_scaled = scaler_key.transform(X_test_key)\n",
    "\n",
    "# Create a simple model with these features\n",
    "# Use GradientBoosting which showed good validation performance earlier\n",
    "gb_key = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=3,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    subsample=0.7,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb_key.fit(X_key_scaled, y_encoded)\n",
    "\n",
    "# Generate predictions\n",
    "final_preds_key = gb_key.predict(X_test_key_scaled)\n",
    "final_preds_labels_key = label_encoder.inverse_transform(final_preds_key)\n",
    "\n",
    "submission_key = pd.DataFrame({\n",
    "    'id': range(1, len(final_preds_labels_key) + 1),\n",
    "    'label': final_preds_labels_key\n",
    "})\n",
    "\n",
    "submission_key.to_csv(\"../outputs/submission_key_markers.csv\", index=False)\n",
    "print(f\"Created submission using only essential biological markers\")\n",
    "\n",
    "print(\"\\nYou now have a total of 7 new submission files to try!\")\n",
    "print(\"1. submission_extreme_reg.csv - XGBoost with extreme regularization\")\n",
    "print(\"2. submission_top_features.csv - XGBoost with only top 10 important features\")\n",
    "print(\"3. submission_gb.csv - GradientBoostingClassifier model\")\n",
    "print(\"4. submission_augmented.csv - XGBoost with data augmentation\")\n",
    "print(\"5. submission_robust_ensemble.csv - Ensemble of diverse models on curated features\")\n",
    "print(\"6. submission_key_markers.csv - GradientBoosting with only essential biological markers\")\n",
    "print(\"7. Your original regularized model for comparison\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
