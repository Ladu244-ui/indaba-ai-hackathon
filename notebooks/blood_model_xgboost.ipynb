{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4742fb23",
   "metadata": {},
   "source": [
    "# ðŸ”¶ Blood Sample Classification Model with XGBoost\n",
    "\n",
    "This notebook implements a robust classification model for blood sample analysis, focusing on simplicity and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d769d4e4",
   "metadata": {},
   "source": [
    "## ðŸ”¶ Step 1: Data Collection and Setup\n",
    "\n",
    "First, we'll load the necessary libraries and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf99e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "train = pd.read_csv(\"../Data/Blood_samples_dataset_balanced_2(f).csv\")\n",
    "test = pd.read_csv(\"../Data/blood_samples_dataset_test.csv\")\n",
    "\n",
    "print(f\"Training data shape: {train.shape}\")\n",
    "print(f\"Testing data shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576325f9",
   "metadata": {},
   "source": [
    "## ðŸ”¶ Step 2: Data Exploration\n",
    "\n",
    "Let's explore the data to understand its structure and detect any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a48c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick overview of the data\n",
    "print(\"\\n=== Target Variable Distribution ===\")\n",
    "print(train['Disease'].value_counts())\n",
    "print(\"\\n=== Missing Values in Train ===\")\n",
    "print(train.isnull().sum().sum())\n",
    "print(\"\\n=== Missing Values in Test ===\")\n",
    "print(test.isnull().sum().sum())\n",
    "\n",
    "# Check for data type issues\n",
    "print(\"\\n=== Data Types ===\")\n",
    "print(train.dtypes)\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\n=== Statistical Summary ===\")\n",
    "print(train.describe().T)\n",
    "\n",
    "# Examine a few samples\n",
    "print(\"\\n=== Sample Data ===\")\n",
    "print(train.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fa65c5",
   "metadata": {},
   "source": [
    "## ðŸ”¶ Step 3: Data Preprocessing\n",
    "\n",
    "We'll prepare the data for modeling by:\n",
    "1. Separating features and target\n",
    "2. Encoding categorical variables\n",
    "3. Handling any issues detected in exploration\n",
    "4. Scaling the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fc3f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature preparation\n",
    "print(\"Preparing features and target...\")\n",
    "\n",
    "# Separate features and target for train data\n",
    "X = train.drop(columns=['Disease'])\n",
    "y = train['Disease']\n",
    "\n",
    "# Prepare test data\n",
    "if 'Disease' in test.columns:\n",
    "    X_test = test.drop(columns=['Disease'])\n",
    "else:\n",
    "    X_test = test.copy()\n",
    "\n",
    "# Check and ensure feature consistency between train and test\n",
    "missing_cols = set(X.columns) - set(X_test.columns)\n",
    "extra_cols = set(X_test.columns) - set(X.columns)\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"Warning: Test data missing columns: {missing_cols}\")\n",
    "if extra_cols:\n",
    "    print(f\"Warning: Test data has extra columns: {extra_cols}\")\n",
    "    X_test = X_test.drop(columns=list(extra_cols))\n",
    "\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Prepared {X_scaled.shape[1]} features for training\")\n",
    "print(f\"Target distribution:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    count = (y_encoded == i).sum()\n",
    "    print(f\"  {label}: {count} ({count/len(y_encoded)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7579765",
   "metadata": {},
   "source": [
    "## ðŸ”¶ Step 4: Model Training\n",
    "\n",
    "We'll use XGBoost, a powerful gradient boosting algorithm known for its effectiveness in classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39ce54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_scaled, y_encoded, \n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    stratify=y_encoded  # Maintain class distribution\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}\")\n",
    "\n",
    "# Train XGBoost classifier with robust parameters\n",
    "print(\"\\nTraining XGBoost classifier...\")\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,          # Number of trees\n",
    "    learning_rate=0.1,         # Learning rate\n",
    "    max_depth=6,               # Maximum tree depth\n",
    "    min_child_weight=1,        # Minimum sum of instance weight needed in a child\n",
    "    subsample=0.8,             # Subsample ratio of training instances\n",
    "    colsample_bytree=0.8,      # Subsample ratio of columns for each tree\n",
    "    objective='multi:softprob', # Multi-class probability\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss'     # Evaluation metric\n",
    ")\n",
    "\n",
    "# Train with early stopping using validation set\n",
    "eval_set = [(X_val, y_val)]\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=eval_set,\n",
    "    early_stopping_rounds=20,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_preds = xgb_model.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, val_preds)\n",
    "print(f\"\\nValidation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Show detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, val_preds, target_names=label_encoder.classes_))\n",
    "\n",
    "# Cross-validation to ensure model stability\n",
    "cv_scores = cross_val_score(\n",
    "    xgb_model, X_scaled, y_encoded,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='accuracy'\n",
    ")\n",
    "print(f\"\\nCross-validation Accuracy: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdac09e",
   "metadata": {},
   "source": [
    "## ðŸ”¶ Step 5: Feature Importance\n",
    "\n",
    "Let's examine which features are most important for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ecd6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "importance = xgb_model.feature_importances_\n",
    "indices = np.argsort(importance)[-20:]  # Top 20 features\n",
    "plt.barh(range(len(indices)), importance[indices])\n",
    "plt.yticks(range(len(indices)), [X.columns[i] for i in indices])\n",
    "plt.title('Top 20 Most Important Features')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top features\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "for i in indices[-10:]:\n",
    "    print(f\"{X.columns[i]}: {importance[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bebdae0",
   "metadata": {},
   "source": [
    "## ðŸ”¶ Step 6: Generate Predictions and Submission File\n",
    "\n",
    "Finally, we'll make predictions on the test data and create a submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4958e9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on test data\n",
    "print(\"Generating predictions on test data...\")\n",
    "test_preds_prob = xgb_model.predict_proba(X_test_scaled)\n",
    "test_preds = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "# Decode predictions to original labels\n",
    "test_preds_labels = label_encoder.inverse_transform(test_preds)\n",
    "\n",
    "# Calculate prediction confidence\n",
    "prediction_confidence = np.max(test_preds_prob, axis=1)\n",
    "\n",
    "# Create submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'id': range(1, len(test_preds_labels) + 1),\n",
    "    'label': test_preds_labels\n",
    "})\n",
    "\n",
    "# Create outputs directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(\"../outputs\", exist_ok=True)\n",
    "\n",
    "# Save submission file\n",
    "submission.to_csv(\"../outputs/submission_xgboost.csv\", index=False)\n",
    "print(f\"Submission file saved with {len(submission)} predictions\")\n",
    "\n",
    "# Print prediction distribution\n",
    "print(\"\\nPrediction Distribution:\")\n",
    "pred_counts = pd.Series(test_preds_labels).value_counts()\n",
    "for label, count in pred_counts.items():\n",
    "    print(f\"  {label}: {count} ({count/len(test_preds_labels)*100:.1f}%)\")\n",
    "\n",
    "# Print confidence statistics\n",
    "print(\"\\nConfidence Statistics:\")\n",
    "print(f\"  Mean: {prediction_confidence.mean():.4f}\")\n",
    "print(f\"  Min: {prediction_confidence.min():.4f}\")\n",
    "print(f\"  Max: {prediction_confidence.max():.4f}\")\n",
    "\n",
    "# Display sample predictions\n",
    "print(\"\\nSample Predictions (first 10):\")\n",
    "sample_df = pd.DataFrame({\n",
    "    'id': range(1, 11),\n",
    "    'predicted_label': test_preds_labels[:10],\n",
    "    'confidence': prediction_confidence[:10]\n",
    "})\n",
    "print(sample_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
